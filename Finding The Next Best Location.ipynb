{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516fd35b",
   "metadata": {},
   "source": [
    "# Location Based Business Reccomendation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "763be30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import folium\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaa6e0",
   "metadata": {},
   "source": [
    "## Neighborhood Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34482895",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'api.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     CLIENT_ID \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      3\u001b[0m     CLIENT_SECRET \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\u001b[38;5;241m.\u001b[39mstrip()    \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'api.txt'"
     ]
    }
   ],
   "source": [
    "with open('api.txt') as f:\n",
    "    CLIENT_ID = f.readline().strip()\n",
    "    CLIENT_SECRET = f.readline().strip()    \n",
    "\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "LIMIT = 100 # A default Foursquare API limit value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_latitude = present_stores.loc[0, 'latitude'] # neighborhood latitude value\n",
    "neighborhood_longitude = present_stores.loc[0, 'longitude'] # neighborhood longitude value\n",
    "neighborhood_name = present_stores.loc[0, 'address'] # neighborhood name\n",
    "\n",
    "print('Latitude and longitude values of {} are {}, {}.'.format(neighborhood_name, \n",
    "                                                               neighborhood_latitude, \n",
    "                                                               neighborhood_longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca969cb",
   "metadata": {},
   "source": [
    "## Import and Clean Metro Data from Census\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df = pd.read_csv('DP03_All_Metros.csv')\n",
    "metro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df.dtypes\n",
    "metro_df[metro_df['geographic area name'] == 'Fort Payne']\n",
    "#The API returns all Null Values as -9999999999.0, convert them to label average\n",
    "metro_df.replace(-999999999.0, np.nan, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in metro_df.columns[4:]:\n",
    "    metro_df[column].replace(np.nan, metro_df[column].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a15dc",
   "metadata": {},
   "source": [
    "##  Add Geospatial Data To Metro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a314e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df[['geographic area name', 'state']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = ArcGIS(user_agent = 'IBM_DS_Capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b86e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locateMetroCities(cities, states):\n",
    "    '''\n",
    "    cities: str, name of metro. Eg: \"Seattle-Tacoma-Bellevue\", or \"Idaho Falls\"\n",
    "    states: str, state codes. Eg: \"ID-WA\", or \"OR\"\n",
    "    '''\n",
    "    \n",
    "    city_list = cities.split('-')\n",
    "    state_list = states.split('-')\n",
    "    \n",
    "    lat_long_list = []\n",
    "    \n",
    "    for city in city_list:\n",
    "        \n",
    "        location = geolocator.geocode(city + ', ' + state_list[0])\n",
    "        \n",
    "        #makes sure we have the right city in the right state\n",
    "        if (location == None) or (location.address.split(', ')[0] != city):\n",
    "            \n",
    "            i = 1\n",
    "            while (i < len(state_list)) and (location == None):\n",
    "                \n",
    "                location = geolocator.geocode(city + ', ' + state_list[i])\n",
    "                i += 1\n",
    "        \n",
    "        lat_long_list.append([location.latitude, location.longitude])\n",
    "    \n",
    "    return lat_long_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = metro_df.loc[0:74].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaac493",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long2 = metro_df.loc[75:148].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long3 = metro_df.loc[149:222].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long4 = metro_df.loc[223:296].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long5 = metro_df.loc[297:370].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b5734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long6 = metro_df.loc[371:444].apply(lambda x: locateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long7 = metro_df.loc[445:519].apply(lambda x: zlocateMetroCities(x['geographic area name'], x['state']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lat_long), len(lat_long2), len(lat_long3), len(lat_long4), len(lat_long5), len(lat_long6), len(lat_long7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = pd.concat([lat_long, lat_long2, lat_long3, lat_long4, lat_long5, lat_long6, lat_long7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cca04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long.to_csv('lat_long.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c813fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = pd.read_csv('lat_long.csv', header = None)\n",
    "lat_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67efe8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df.insert(5, 'lat_long', lat_long)\n",
    "metro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9feb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long_str_to_list_of_floats(lat_long_str):\n",
    "    \n",
    "    lat_long_str = lat_long_str.replace('[', '').replace(']', '')    \n",
    "    lat_long_list = lat_long_str.split(',')\n",
    "    \n",
    "    lat_long_pairs_count =  int(len(lat_long_list) / 2)    \n",
    "    lat_long_pairs = [[] for i in range(lat_long_pairs_count)]\n",
    "    \n",
    "    count = 0\n",
    "    for pair in lat_long_pairs:\n",
    "        \n",
    "        pair.append(float(lat_long_list[0 + count]))\n",
    "        pair.append(float(lat_long_list[1 + count]))\n",
    "        count += 2\n",
    "        \n",
    "    return lat_long_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_df['lat_long'] = pd.Series(map(lat_long_str_to_list_of_floats, metro_df['lat_long']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4db52",
   "metadata": {},
   "source": [
    "## Regions of Interest + FourSquare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4191e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.foursquare.com/v2/venues/categories?&client_id={}&client_secret={}&v={}'.format(\n",
    "                CLIENT_ID, \n",
    "                CLIENT_SECRET,\n",
    "                VERSION)\n",
    "\n",
    "categories = requests.get(url).json()\n",
    "\n",
    "food_category_dict = categories['response']['categories'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dict_extract(key, var):\n",
    "    '''\n",
    "    https://stackoverflow.com/questions/9807634/find-all-occurrences-of-a-key-in-nested-dictionaries-and-lists\n",
    "    Title: Find all occurrences of a key in nested dictionaries and lists\n",
    "    by user: hexerei software\n",
    "    28 MAY 2020\n",
    "    \n",
    "    Pulls all values with a specific key from a nested dictionary\n",
    "    'var' is the dict\n",
    "    \n",
    "    '''\n",
    "    if hasattr(var,'items'):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, dict):\n",
    "                for result in gen_dict_extract(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, list):\n",
    "                for d in v:\n",
    "                    for result in gen_dict_extract(key, d):\n",
    "                        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(gen_dict_extract('id', food_category_dict))\n",
    "name_list = list(gen_dict_extract('name', food_category_dict))\n",
    "\n",
    "#remove generic 'Food' category\n",
    "id_list = id_list[1:]\n",
    "name_list = name_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_id_dict = {}\n",
    "id_name_dict = {}\n",
    "for i in range(0,len(name_list)):\n",
    "    name_id_dict[name_list[i]] = id_list[i]\n",
    "    id_name_dict[id_list[i]] = name_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ecf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRestaurantCoordsAndType(lat_long_list, category_list, limit = 50, radius = 25000, redo50 = False):\n",
    "\n",
    "    allVenues_df = pd.DataFrame(columns = ['name', 'category', 'category_id', 'id', 'lat', 'long'])\n",
    "    \n",
    "    for lat_long in lat_long_list:\n",
    "        \n",
    "        for category in category_list:\n",
    "\n",
    "            lat = lat_long[0]\n",
    "            long = lat_long[1]\n",
    "\n",
    "            url = 'https://api.foursquare.com/v2/venues/explore?&openNow=0&time=any&sortByPopularity=1&categoryId=' + category + '&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "                        CLIENT_ID, \n",
    "                        CLIENT_SECRET, \n",
    "                        VERSION, \n",
    "                        lat, \n",
    "                        long, \n",
    "                        radius, \n",
    "                        limit)\n",
    "            \n",
    "            result = requests.get(url).json()\n",
    "            \n",
    "            #Keep relevant data only\n",
    "            try:\n",
    "                #Foursquare will only return a max of 50 venues.\n",
    "                #If the max is reached, this section splits the initial radial search into several smaller searches\n",
    "                if 'groups' in result['response'] and len(result['response']['groups'][0]['items']) == 50 and redo50 == False:\n",
    "                    \n",
    "                    lat_long_expanded_list = []\n",
    "                    degree_list = [0, 45, 90, 135, 180, 225, 270, 315]                    \n",
    "                    for degree in degree_list:\n",
    "                        if degree % 90 == 0:\n",
    "                            distance = radius/2\n",
    "                        else:\n",
    "                            distance = radius * (2/5)\n",
    "                        origin = Point(lat, long)\n",
    "                        lat_long_expanded_list.append(list(geodesic(meters = distance).destination(origin, degree))[0:2])\n",
    "                    \n",
    "                    allVenues_df = pd.concat([allVenues_df, getRestaurantCoordsAndType(lat_long_expanded_list, [category], limit = 50, radius = radius/2, redo50 = True)], sort = False)\n",
    "                    \n",
    "                elif 'groups' in result['response'] and len(result['response']['groups'][0]['items']) > 0:\n",
    "                    \n",
    "                    venues_df = pd.DataFrame(list(gen_dict_extract('venue', result['response']['groups'][0])))\n",
    "                    \n",
    "                    venues_df.drop(venues_df.loc[venues_df['categories'].map(lambda x: len(x)== 0)].index, inplace = True)                    \n",
    "                    venues_df['category'] = venues_df['categories'].map(lambda x: x[0]['name'])\n",
    "                    venues_df['category_id'] = venues_df['categories'].map(lambda x:x[0]['id'])\n",
    "                    \n",
    "                    venues_df['lat'] = venues_df['location'].map(lambda x: x['lat'])\n",
    "                    venues_df['long'] = venues_df['location'].map(lambda x: x['lng'])\n",
    "                    \n",
    "                    venues_df.drop(['categories','location', 'photos'], axis = 1, inplace = True)\n",
    "\n",
    "                    if 'venuePage' in venues_df:\n",
    "                        venues_df.drop(['venuePage'], axis = 1, inplace = True)\n",
    "                    if 'delivery' in venues_df:\n",
    "                        venues_df.drop(['delivery'], axis = 1, inplace = True)\n",
    "                    if 'events' in venues_df:\n",
    "                        venues_df.drop(['events'], axis = 1, inplace = True)\n",
    "                    \n",
    "                    allVenues_df = pd.concat([allVenues_df, venues_df], sort = False)\n",
    "                    \n",
    "            except KeyError as err:\n",
    "\n",
    "                print('KeyError for cat: ' + category + ' at ' + str(lat) + ', ' + str(long))\n",
    "                traceback.print_exc()\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            except IndexError as err:\n",
    "                \n",
    "                print('IndexError for cat: ' + category + ' at ' + str(lat) + ', ' + str(long))\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            except TypeError as err:\n",
    "                \n",
    "                print('TypeError for cat: ' + category + ' at ' + str(lat) + ', ' + str(long))\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    allVenues_df.drop_duplicates(subset = 'id', keep = 'last', inplace = True)\n",
    "    \n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Gas Station']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Grocery Store']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Food Court']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Food']\n",
    "    #allVenues_df.drop(allVenues_df.loc[allVenues_df['category'] == 'Fast Food Restaurant'].index, inplace = True)\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Fast Food Restaurant']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Coffee Shop']\n",
    "    allVenues_df = allVenues_df[allVenues_df['category'] != 'Shopping Plaza']\n",
    "    \n",
    "    if redo50 == False:\n",
    "        allVenues_df.rename(columns = {'id' : 'venue_id'}, inplace = True)\n",
    "    allVenues_df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return allVenues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c3242",
   "metadata": {},
   "source": [
    "### foursquare API call usage batching 5000 calls per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict = {}\n",
    "metro_venue_dict2 = {}\n",
    "metro_venue_dict3 = {}\n",
    "metro_venue_dict4 = {}\n",
    "metro_venue_dict5 = {}\n",
    "\n",
    "print('Starting batch 1 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[0 : 4].index:    \n",
    "    metro_venue_dict[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 2 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[5 : 9].index:    \n",
    "    metro_venue_dict2[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 3 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[10 : 14].index:    \n",
    "    metro_venue_dict3[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 4 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[15 : 19].index:    \n",
    "    metro_venue_dict4[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 5 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[20 : 24].index:    \n",
    "    metro_venue_dict5[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 6 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[25 : 29].index:    \n",
    "    metro_venue_dict6[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "print('Batch complete. 45min cooldown started at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n\\n')\n",
    "sleep(60*45)\n",
    "\n",
    "print('Starting batch 7 at ' + datetime.now().strftime(\"%H:%M:%S\") + '\\n')\n",
    "for i in PNW_metro_df.loc[30 : 34].index:    \n",
    "    metro_venue_dict7[PNW_metro_df['geographic area name'][i]] = getRestaurantCoordsAndType(PNW_metro_df['lat_long'][i], id_list)\n",
    "\n",
    "    \n",
    "print('Final batch complete at ' + datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict = {**metro_venue_dict, **metro_venue_dict2, **metro_venue_dict3, **metro_venue_dict4, **metro_venue_dict5, **metro_venue_dict6, **metro_venue_dict7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkdir('./metro_venues')\n",
    "    \n",
    "for key in metro_venue_dict.keys():\n",
    "    metro_venue_dict[key].to_csv('./metro_venues/' + key + '.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df298fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict = {}\n",
    "\n",
    "for file in listdir('./metro_venues'):\n",
    "    metro_venue_dict[file[0:-4]] = pd.read_csv('./metro_venues/' + file)\n",
    "\n",
    "metro_venue_dict['Aberdeen'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce73ad9",
   "metadata": {},
   "source": [
    "## K Means Clustering For Restaurant Type Frequency Per Metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997457fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_frequency_df = pd.DataFrame(name_list)\n",
    "rest_frequency_df.set_index(0, inplace = True)\n",
    "del rest_frequency_df.index.name\n",
    "\n",
    "for key in metro_venue_dict.keys():\n",
    "    \n",
    "    rest_frequency_df[key] = metro_venue_dict[key]['category'].value_counts()\n",
    "\n",
    "rest_frequency_df.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "rest_frequency_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd45375",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "frequency_normed_df = pd.DataFrame(min_max_scaler.fit_transform(rest_frequency_df))\n",
    "\n",
    "frequency_normed_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309db651",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters = k).fit(frequency_normed_df.transpose())\n",
    "kmeans.labels_[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09173f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    PNW_metro_df.insert(0, 'cluster', kmeans.labels_)\n",
    "except ValueError:\n",
    "    PNW_metro_df['cluster'] = kmeans.labels_\n",
    "    \n",
    "PNW_metro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "PNW_metro_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_clusters = folium.Map(width = '100%', height = '100%', location = [45.3, -118], zoom_start = 5)\n",
    "\n",
    "x = np.arange(k)\n",
    "ys = [i + x + (i*x)**2 for i in range(k)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "\n",
    "for i in PNW_metro_df.index:\n",
    "    lat = PNW_metro_df['lat_long'][i][0][0]\n",
    "    long = PNW_metro_df['lat_long'][i][0][1]\n",
    "    metro = PNW_metro_df['geographic area name'][i]\n",
    "    pop = PNW_metro_df['population_16_up'][i]\n",
    "    cluster = PNW_metro_df['cluster'][i]\n",
    "    \n",
    "    label = folium.Popup(str(metro) + '\\nPopulation: ' + str(pop) + '\\nCluster: ' + str(cluster), parse_html=True)\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        [lat, long],\n",
    "        radius=3,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd414c",
   "metadata": {},
   "source": [
    "## Statistics for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mean_std = [[[],[]], [[],[]], [[],[]]]\n",
    "\n",
    "cluster_mean_std[0][0] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']].mean(axis = 1)\n",
    "cluster_mean_std[0][1] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']].std(axis = 1)\n",
    "\n",
    "cluster_mean_std[1][0] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']].mean(axis = 1)\n",
    "cluster_mean_std[1][1] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']].std(axis = 1)\n",
    "\n",
    "cluster_mean_std[2][0] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']].mean(axis = 1)\n",
    "cluster_mean_std[2][1] = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']].std(axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1410b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_mean = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']].mean(axis = 1)\n",
    "cluster1_mean = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']].mean(axis = 1)\n",
    "cluster2_mean = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']].mean(axis = 1)\n",
    "\n",
    "cluster0_std = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']].std(axis = 1)\n",
    "cluster1_std = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']].std(axis = 1)\n",
    "cluster2_std = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']].std(axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7934c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_df = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 0]['geographic area name']]\n",
    "cluster1_df = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 1]['geographic area name']]\n",
    "cluster2_df = frequency_normed_df[PNW_metro_df[PNW_metro_df['cluster'] == 2]['geographic area name']]\n",
    "\n",
    "cluster0_df.insert(0,'cluster_std', cluster0_std)\n",
    "cluster0_df.insert(0,'cluster_mean', cluster0_mean)\n",
    "cluster1_df.insert(0,'cluster_std', cluster1_std)\n",
    "cluster1_df.insert(0,'cluster_mean', cluster1_mean)\n",
    "cluster2_df.insert(0,'cluster_std', cluster2_std)\n",
    "cluster2_df.insert(0,'cluster_mean', cluster2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c96946",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index cluster dataframes by the restaruant type\n",
    "cluster0_df.index = venue_frequency_df.index\n",
    "cluster1_df.index = venue_frequency_df.index\n",
    "cluster2_df.index = venue_frequency_df.index\n",
    "\n",
    "cluster0_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6004447",
   "metadata": {},
   "source": [
    "## How many standard Deviations is each restaurant type frequency from the mean in each region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850754fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract mean from frequecy\n",
    "std0_df = cluster0_df[cluster0_df.columns[2:23]].subtract(cluster0_df['cluster_mean'], axis = 0)\n",
    "std1_df = cluster1_df[cluster1_df.columns[2:23]].subtract(cluster1_df['cluster_mean'], axis = 0)\n",
    "std2_df = cluster2_df[cluster2_df.columns[2:23]].subtract(cluster2_df['cluster_mean'], axis = 0)\n",
    "\n",
    "#divide by std\n",
    "std0_df = std0_df[std0_df.columns[:]].div(cluster0_df['cluster_std'], axis = 0)\n",
    "std1_df = std1_df[std1_df.columns[:]].div(cluster1_df['cluster_std'], axis = 0)\n",
    "std2_df = std2_df[std2_df.columns[:]].div(cluster2_df['cluster_std'], axis = 0)\n",
    "\n",
    "std_list[0] = std0_df\n",
    "std_list[1] = std1_df\n",
    "std_list[2] = std2_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f237723",
   "metadata": {},
   "outputs": [],
   "source": [
    "std0_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_list[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b1372",
   "metadata": {},
   "source": [
    "## Make reccomendations on metropolitian regions for each restaurant\n",
    " by population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae83f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_venue_ratio_list = []\n",
    "\n",
    "for metro in PNW_metro_df['geographic area name']:\n",
    "    \n",
    "    pop_venue_ratio_list.append(float(PNW_metro_df[PNW_metro_df['geographic area name'] == metro]['population_16_up']) / len(metro_venue_dict[metro]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9809fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PNW_metro_df.insert(5, 'pop_venue_ratio', pop_venue_ratio_list)\n",
    "\n",
    "PNW_metro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7964d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pop_venue_ratio_df = PNW_metro_df.nlargest(3, [\"pop_venue_ratio\"], keep = 'all')[['cluster', 'geographic area name', 'pop_venue_ratio']]\n",
    "top_pop_venue_ratio_df.reset_index(inplace = True, drop = True)\n",
    "top_pop_venue_ratio_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6878b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(std_list[top_pop_venue_ratio_df.iloc[0]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[0]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[0]['geographic area name']], '\\n')\n",
    "print(std_list[top_pop_venue_ratio_df.iloc[1]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[1]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[1]['geographic area name']], '\\n')\n",
    "print(std_list[top_pop_venue_ratio_df.iloc[2]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[2]['geographic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f752f0",
   "metadata": {},
   "source": [
    "### get best locations for top under represented locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_underrepresented = [[], [], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_underrepresented[0] = std_list[top_pop_venue_ratio_df.iloc[0]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[0]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[0]['geographic area name']].index.values\n",
    "top_underrepresented[1] = std_list[top_pop_venue_ratio_df.iloc[1]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[1]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[1]['geographic area name']].index.values\n",
    "top_underrepresented[2] = std_list[top_pop_venue_ratio_df.iloc[2]['cluster']].nsmallest(3, [top_pop_venue_ratio_df.iloc[2]['geographic area name']], keep = 'all')[top_pop_venue_ratio_df.iloc[2]['geographic area name']].index.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e19f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']]['intrametro cluster'] = 'not yet'\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']]['intrametro cluster'] = 'not yet'\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']]['intrametro cluster'] = 'not yet'\n",
    "\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']].head()\n",
    "\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']]['intrametro cluster'].value_counts()['not yet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb5b551",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metro_venue_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m resto_lat_long_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m resto_lat_long_list\u001b[38;5;241m.\u001b[39mappend(metro_venue_dict[top_pop_venue_ratio_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeographic area name\u001b[39m\u001b[38;5;124m'\u001b[39m]][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy())\n\u001b[1;32m      3\u001b[0m resto_lat_long_list\u001b[38;5;241m.\u001b[39mappend(metro_venue_dict[top_pop_venue_ratio_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeographic area name\u001b[39m\u001b[38;5;124m'\u001b[39m]][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy())\n\u001b[1;32m      4\u001b[0m resto_lat_long_list\u001b[38;5;241m.\u001b[39mappend(metro_venue_dict[top_pop_venue_ratio_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeographic area name\u001b[39m\u001b[38;5;124m'\u001b[39m]][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metro_venue_dict' is not defined"
     ]
    }
   ],
   "source": [
    "resto_lat_long_list = []\n",
    "resto_lat_long_list.append(metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']][['lat', 'long']].to_numpy())\n",
    "resto_lat_long_list.append(metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']][['lat', 'long']].to_numpy())\n",
    "resto_lat_long_list.append(metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']][['lat', 'long']].to_numpy())\n",
    "eps = 0.004\n",
    "\n",
    "dbsacn_list = []\n",
    "dbsacn_list.append(DBSCAN(eps = eps, min_samples = 5).fit(resto_lat_long_list[0]))\n",
    "dbsacn_list.append(DBSCAN(eps = eps, min_samples = 5).fit(resto_lat_long_list[1]))\n",
    "dbsacn_list.append(DBSCAN(eps = eps, min_samples = 5).fit(resto_lat_long_list[2]))\n",
    "\n",
    "dbsacn_list[0].labels_[0:4]\n",
    "\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']]['intrametro cluster'] = dbsacn_list[0].labels_\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[1]['geographic area name']]['intrametro cluster'] = dbsacn_list[1].labels_\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[2]['geographic area name']]['intrametro cluster'] = dbsacn_list[2].labels_\n",
    "\n",
    "metro_venue_dict[top_pop_venue_ratio_df.iloc[0]['geographic area name']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03c11b",
   "metadata": {},
   "source": [
    "### top 10 most populated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23986c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMostPopulatedClusters(metro, topXclusters = 10, viewOutliers = False):\n",
    "    \n",
    "    cluster_value_counts = pd.DataFrame(metro_venue_dict[metro]['intrametro cluster'].value_counts())\n",
    "    clusters = list(cluster_value_counts[0:topXclusters].index)\n",
    "\n",
    "    #Remove -1 (outliers) from clusters\n",
    "    if -1 in clusters and viewOutliers == False:   \n",
    "        outlier_index = clusters.index(-1)\n",
    "        \n",
    "        if len(cluster_value_counts) > topXclusters:\n",
    "            clusters[outlier_index] = cluster_value_counts.index[topXclusters]\n",
    "        else:\n",
    "            clusters.pop(outlier_index)\n",
    "        \n",
    "    return clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
